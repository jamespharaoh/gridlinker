## Physical server

Order the server with Hetzner. It needs to be placed in the dedicated rack space
and the second network port should be connected to the private switch. It should
have two SSDs by default and a 4TB spinning disk.

Once the server is made available, an extra regular IP address should be added,
for the virtual machine, and a failover address, for inbound traffic.

The forward and reverse DNS should be set up as follows. The forward DNS is
configured via cloudfront, and the reverse DNS is configured via the Hetzner
Robot.

| Name                              | Description  |
| --------------------------------- | ------------ |
| rocket-hz-host-X.rocketware.co.uk | Primary IP   |
| rocket-hz-vm-X.rocketware.co.uk   | Secondary IP |
| rocket-hz-pub-X.rocketware.co.uk  | Failover IP  |

The system should be booted into rescue mode, and a password will be provided.
SSH into the host and perform the base installation:

```
name="rocket-hz-host-X"
drive="sda,sdb"
root_type="ext4"
root_size="32"
version="1604-xenial"

images="/root/.oldroot/nfs/install/../images"
image="$images/Ubuntu-${version}-64-minimal.tar.gz"

installimage -a -n ${name} -i ${image} \
    -d ${drive} -r yes -l 1 -b grub \
    -p /:${root_type}:${root_size}G
```

This should take a minute or two, then you can reboot the machine. Once it
restarts you can connect using the same credentials. You will need to remove the
SSH host key, both for the hostname and IP address, using `ssh-keygen -R`, since
this will change when the new system boots for the first time.

Now, perform some basic system configuration and enable SSH access via public
key.

```
apt update
apt dist-upgrade
apt install python

mkdir -p /root/.ssh
cat >> /root/.ssh/authorized_keys
(paste your public SSH key here)
(type control+D, then enter on a new line to end)
```

Now reboot again, since the kernel and other system packages are likely to have
been updated. Verify that the SSH public key access is working.

At this point, we can continue the server configuration using ansible from a
devbox. Create a resource for the new server:

```
./datingnode-master resource create \
    --class "rocket-hz-host" \
    --name "rocket-hz-host-X" \
    --set "identity.index" "X" \
    --set "ubuntu.release" "xenial" \
    --set "system.init" "systemd" \
    --set "private.address" "x.x.x.x" \
    --set "private.broadcast" "10.69.131.255" \
    --set "private.interface" "eth1" \
    --set "private.netbits" "22" \
    --set "private.netmask" "255.255.252.0" \
    --set "private.network" "10.69.128.0" \
    --set "public.address" "x.x.x.x" \
    --set "public.broadcast" "x.x.x.x" \
    --set "public.gateway" "x.x.x.x" \
    --set "public.interface" "eth0" \
    --set "public.netbits" "xx" \
    --set "public.netmask" "x.x.x.x" \
    --set "public.network" "x.x.x.x"
```

And run the playbook to configure it:

```
./datingnode-master ansible playbook -- \
    playbooks/rocket-hz-host.yml \
    --limit host/rocket-hz-host-X
```

This will fail when attempting to start `dnsmasq`. This is because it is trying
to bind to the bridge interface `brprv0` which has been configured but not
enabled. Reboot the server and run the script again.

```
./datingnode-master ansible playbook -- \
    playbooks/rocket-hz-host.yml \
    --limit host/rocket-hz-host-X
```

It will fail again when trying to start `etcd`. At this point, add the new
server as a cluster member:

```
./datingnode-master etcd control -- \
    member add rocket-hz-host-X \
    https://rocket-hz-host-X.rocketware.co.uk:2380
```

This will output a set of environment variables named `ETCD_NAME`,
`ETCD_INITIAL_CLUSTER` and `ETCD_INITIAL_CLUSTER_STATE`. Copy this information
and paste it to the end of `/etc/etcd/etcd-environment` on the host, then reset
and start `etcd`:

```
systemctl stop etcd
rm -rf /var/lib/etcd/member
```

Back on the devbox, confirm that the cluster has been updated correctly and is
healthy:

```
./datingnode-master etcd control -- cluster-health
```

Add the new etcd server in `config/connections.yml` and `data/project, then run
the playbook again:

```
./datingnode-master ansible playbook -- \
    playbooks/rocket-hz-host.yml \
    --limit host/rocket-hz-host-X
```

The playbook will fail again when it tried to configure `openvpn`. Issue a new
server certificate:

```
./datingnode-master certificate authority issue \
    --authority "openvpn" \
    --common-name "rocket-hz-host-X" \
    --server
```

Note the serial number and configure the host resource as appropriate. You'll
also need to allocate a unique network for the VPN clients. This example uses
the letter "Y" to represent the serial number:

```
./datingnode-master resource update \
    --name "host/rocket-hz-host-X" \
    --set "openvpn.certificate" "/authority/openvpn/issue/Y/certificate" \
    --set "openvpn.private_key" "/authority/openvpn/issue/Y/key" \
    --set "openvpn.network" "10.69.x"
```

Run the playbook again:

```
./datingnode-master ansible playbook -- \
    playbooks/rocket-hz-host.yml \
    --limit host/rocket-hz-host-X
```

This time the playbook should complete. We now need to update some configuration
on the other servers:

TODO

Finally, we'll partition the disks and setup LVM:

```
parted /dev/sda
unit mb
print
mkpart primary ext2 34361 -0
toggle 2 lvm
quit

parted /dev/sdb
unit mb
print
mkpart primary ext2 34361 -0
toggle 2 lvm
quit

gdisk /dev/sdc
n
1
2048
-0
8e00
w
y

pvcreate /dev/sda2 /dev/sdb2 /dev/sdc1

vgcreate lvm-fast /dev/sda2 /dev/sdb2
vgcreate lvm-big /dev/sdc1

lvcreate lvm-fast --thinpool thinpool --size 512GiB --poolmetadatasize 16GiB
lvcreate lvm-fast --name bcache --size 256GiB

make-bcache --cache /dev/lvm-fast/bcache
```

## Virtual machine

Create logical volumes and filesystems:

```
lvcreate lvm-fast/thinpool --name rocket-hz-vm-X --virtualsize 64GiB

lvcreate lvm-fast/thinpool --name docker-X --virtualsize 64GiB
mkfs.btrfs /dev/lvm-fast/docker-X --label docker-X

lvcreate lvm-fast/thinpool --name lxc-X --virtualsize 64GiB
mkfs.btrfs /dev/lvm-fast/lxc-X --label lxc-X

lvcreate lvm-big --name live-backups-X --size 512GiB
make-bcache --bdev --cset-uuid xxxx --discard /dev/lvm-big/live-backups-X
mkfs.btrfs /dev/bcache0 --label live-backups-X

lvcreate lvm-fast/thinpool --name live-cache-X --virtualsize 128GiB
mkfs.ext4 /dev/lvm-fast/live-cache-X -L live-cache-X

lvcreate lvm-fast/thinpool --name live-database-X --virtualsize 192GiB
mkfs.ext4 /dev/lvm-fast/live-database-X -L live-database-X

lvcreate lvm-big --name live-images-X --size 2TiB
make-bcache --cache --cset-uuid xxxx --discard /dev/lvm-big/live-images-X
mkfs.btrfs /dev/bcache1 --label live-images-X
mkdir -p /tmp/live-images-4
mount /dev/bcache1 /tmp/live-images-4
btrfs subvolume create /tmp/live-images-4/live
btrfs subvolume create /tmp/live-images-4/staging
umount /tmp/live-images-4
rmdir /tmp/live-images-4

lvcreate lvm-big --name staging-cache-X --size 128GiB
make-bcache --cache --cset-uuid xxxx --discard /dev/lvm-big/staging-cache-X
mkfs.btrfs /dev/bcache2 --label staging-cache-X

lvcreate lvm-big --name staging-database-X --size 192GiB
make-bcache --cache --cset-uuid xxxx --discard /dev/lvm-big/staging-database-X
mkfs.btrfs /dev/bcache3 --label staging-database-X
```

Download the Ubuntu server ISO image and store it on the host under a directory
called `/iso`. Create a file for the virtual machine definition, using the
following template. Make sure to put the server index in place of `X`, the
correct path to the downloaded ISO, and the MAC address which corresponds to the
allocated hardware MAC address for the machine's IP address by Hetzner.

```xml
<domain type='kvm'>
  <name>rocket-hz-vm-X</name>
  <memory unit='GiB'>224</memory>
  <currentMemory unit='GiB'>224</currentMemory>
  <vcpu placement='static'>12</vcpu>
  <os>
    <type arch='x86_64' machine='q35'>hvm</type>
    <boot dev='hd'/>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>

    <emulator>/usr/bin/kvm-spice</emulator>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/lvm-fast/rocket-hz-vm-X'/>
      <target dev='sda' bus='scsi'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/lvm-fast/lxc-X'/>
      <target dev='sdb' bus='scsi'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/lvm-fast/docker-X'/>
      <target dev='sdc' bus='scsi'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/lvm-fast/live-database-X'/>
      <target dev='sdd' bus='scsi'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/lvm-fast/live-cache-X'/>
      <target dev='sde' bus='scsi'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/bcache0'/>
      <target dev='sdf' bus='scsi'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/bcache1'/>
      <target dev='sdg' bus='scsi'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/bcache2'/>
      <target dev='sdh' bus='scsi'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
      <source dev='/dev/bcache3'/>
      <target dev='sdi' bus='scsi'/>
    </disk>

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/iso/ubuntu-16.04.2-server-amd64.iso'/>
      <target dev='sdj' bus='sata'/>
      <readonly/>
    </disk>

    <interface type='network'>
      <source network='public'/>
      <model type='virtio'/>
      <mac address='xx:xx:xx:xx:xx:xx'/>
    </interface>
    <interface type='network'>
      <source network='private'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>
```

Define and start the VM:

```sh
virsh define rocket-hz-vm-X.xml
virsh start rocket-hz-vm-X
```

Connect using VNC and complete the installation. You should use an SSH port
tunnel to obtain VNC access. To create a tunnel, for example:

```sh
ssh rocket-hz-host-X.rocketware.co.uk -L 5950:localhost:5900
```

Then connect to VNC via the forwarded port on localhost, for example:

```sh
remote-viewer vnc://localhost:5950
```

Here's a list of answers to questions during the install process:

* Full name for new user: `Ubuntu`
* Password for new user: (generate a secure random password)
* Encrypt your home directory: `no`
* Timezone: `UTC`
* Partitioning: `manual`
* Partition disk: `SCSI1` aka `sda`
* Create a new empty partition table on this device: `yes`
* Partition free space on: `SCSI1` aka `sda`
* Create a new partition
* New partition size: (default)
* Type for the new partition: `primary`
* Use as: `ext4`
* Mount point: `/`
* Done setting up the partition
* Finish partitioning and write changes to disk
* Return to configure swap space: `no`
* Write the changes to disk: `yes`
* HTTP proxy information: (empty)
* Automatic updates: `No automatic updates`
* Software selection, enable: `OpenSSH server`

When the installation is complete, stop the machine with `virsh shutdown`, edit
it with `virsh edit`, and remove the `<boot dev='cdrom'/>` and the `<disk>`
which corresponds to the CDROM, then start it again with `virsh start`.

Copy your SSH public key into the host using the password, install it in
`.ssh/authorized_keys`, then remove the user's password with (TODO).

Allow users in the `sudo` group to run `sudo` without entering their password,
by editing `/etc/sudoers`:

```sh
sudo nano /etc/sudoers
```

The new entry should look like this:

```
# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) NOPASSWD: ALL
```

Finally, install any updates, and install python, which is required by ansible:

```sh
sudo apt update
sudo apt dist-upgrade
sudo apt install python
```

Now, in a devbox, create the resource for the VM. You'll need to allocate a
private address and fill in the details for the public address:

```
./datingnode-master resource create \
    --class "rocket-hz-vm" \
    --name "rocket-hz-vm-X" \
    --set-parent "rocket-hz-host-X" \
    --set "identity.index" "X" \
    --set "ubuntu.release" "xenial" \
    --set "system.init" "systemd" \
    --set "private.address" "x.x.x.x" \
    --set "private.broadcast" "10.69.131.255" \
    --set "private.interface" "enp2s2" \
    --set "private.netbits" "22" \
    --set "private.netmask" "255.255.252.0" \
    --set "private.network" "10.69.128.0" \
    --set "public.address" "x.x.x.x" \
    --set "public.broadcast" "x.x.x.x" \
    --set "public.gateway" "x.x.x.x" \
    --set "public.interface" "enp2s1" \
    --set "public.mac" "xx:xx:xx:xx:xx" \
    --set "public.netbits" "xx" \
    --set "public.netmask" "x.x.x.x" \
    --set "public.network" "x.x.x.x"
```

